---
layout: post
author: Mark Hobbs
title: Accelerating peridynamic simulations using Numba CUDA
draft: True
---

This post details the acceleration of the [`pypd`](https://github.com/mark-hobbs/pypd) package using `CUDA`. The goal is to ensure that the code requires no changes to the user interface while automatically detecting the presence of a CUDA-enabled device.

The majority of the simulation time is spent in two functions `particles.compute_forces()` and `particles.update_positions()`. These have both been optimised using `numba` in a manner analagous to shared-memory parallelism with `OpenMP`. The existing implementation does not support distributed-memory architectures (e.g., `MPI`), and as such, the maximum problem size is limited by the memory capacity of a single CPU - typically allowing for simulations of 1 to 2 million particles.

We are going to employ [`numba-cuda`](https://github.com/NVIDIA/numba-cuda) to make the CUDA implementation as easy and seamless as possible.

## The CUDA programming model

Before writing the CUDA kernel, we briefly review the CUDA programming model. The CUDA programming model provides an abstraction of the GPU architecture that acts as a bridge between an application and its possible implementation on GPU hardware. See this [post](https://developer.nvidia.com/blog/cuda-refresher-cuda-programming-model/) for further details.

```bash
CUDA Device Information:
----------------------------------------
CUDA Runtime Version:     12.5
Device Name:              b'Tesla T4'
Compute Capability:       (7, 5)

Memory:
Total Memory:             15.83 GB
Free Memory:              15.72 GB

Compute Resources:
Multiprocessors:          40
Max Threads per Block:    1024

Grid Limitations:
Max Grid Dimensions X:    2147483647
Max Grid Dimensions Y:    65535
Max Grid Dimensions Z:    65535

Additional Characteristics:
Warp Size:                32
Clock Rate:               1.59 GHz
Memory Clock Rate:        5.00 GHz
```

#### 1. Device (GPU)

- Physical hardware accelerator
- Contains multiple Streaming Multiprocessors (SMs)
- Manages global memory and computation resources

#### 2. Streaming Multiprocessors (SMs)

- Core computational units of the GPU
- Contains:
  - CUDA Cores
  - Shared Memory
  - Register File
  - Special Function Units
  - Warp Schedulers

#### 3. Threads

- Smallest unit of execution
- Analogous to a single computational task
- Organised into blocks and grids

#### 4. Thread Blocks

- Collection of threads that can:
  - Share shared memory
  - Synchronise with each other
  - Execute on a single Streaming Multiprocessor (SM)

#### 5. Grid

- Highest-level organisational unit
- Collection of thread blocks
- All blocks in a grid execute the same kernel

### Memory Hierarchy

#### Global Memory

- Largest, slowest memory
- Accessible by all threads
- Resides on device DRAM
- High latency, low bandwidth

#### Shared Memory

- On-chip memory within an SM
- Explicitly managed by programmer
- Much faster than global memory
- Shared by all threads in a block

#### Register Memory

- Fastest memory
- Private to each thread
- Limited in size
- Located directly on the SM

#### Constant Memory

- Read-only cache
- Optimised for broadcast scenarios
- Good for data accessed by multiple threads

#### Texture Memory

- Read-only cache
- Optimised for 2D spatial locality
- Hardware interpolation support

### Execution Model

#### Warp

- Basic unit of thread execution
- Typically 32 threads
- Executed in SIMT (Single Instruction, Multiple Thread) mode
- All threads in a warp execute the same instruction

#### Occupancy

- Measure of active warps per SM
- Determines computational efficiency
- Influenced by:
  - Kernel resource usage
  - SM capabilities
  - Available registers
  - Shared memory consumption

### Kernel Launch Configuration

#### Threads per Block

- Typically multiples of 32 (warp size)
- Maximum usually 1024 threads
- Constrained by:
  - Available registers
  - Shared memory
  - Compute capability

#### Blocks per Grid

- Determined by total work items
- Limited by device capabilities
- Typically calculated as:
  ```python
  blocks_per_grid = (total_work_items + threads_per_block - 1) // threads_per_block
  ```

## `pypd`

Making some simplifications, setting up and running a simulation using `pypd` is broadly as follows:

```python
import pypd

particles = pypd.ParticleSet(geometry)
bonds = pypd.BondSet(particles)
model = pypd.Model(particles, bonds)
simulation = pypd.Simulation(n_time_steps=5000)
simulation.run(model)
```

When the simulation is run we need to detect if `CUDA` is available and then allocate memory to the device.

```python
from numba import cuda


class Simulation:

    def __init__(self):
        self.cuda_available = cuda.is_available()
        print(f"Is CUDA available: {self.cuda_available}")
        if self.cuda_available:
            get_cuda_device_info()

    def run(self, model):
        """
        Run the simulation
        """
        if self.cuda_available:
            model._allocate_gpu_arrays()

        for _ in range(self.n_time_steps):
            self._single_time_step(model)

    def _single_time_step(self, model):
        """
        Single time step
        """
        model.particles.compute_forces(model.bonds, self.cuda_available)
        model.particles.update_positions(self)

        if model.penetrators:
            for penetrator in model.penetrators:
                penetrator.calculate_penetrator_force(model.particles, self)
```

Device memory allocations and data transfers should typically be done outside the CUDA kernel to:

- Minimise redundant data transfers
- Allow for more efficient memory management
- Enable potential reuse of device-allocated memory across multiple function calls

Modify the arrays in place...

```python
class Model:
    
    def _allocate_gpu_arrays(self):
        from numba import cuda
        cuda.to_device(self.particles.x)
        cuda.to_device(self.particles.u)
        cuda.to_device(self.particles.v)
        cuda.to_device(self.particles.a)
        cuda.to_device(self.particles.f)
        cuda.to_device(self.particles.bc.flag)
        cuda.to_device(self.particles.bc.unit_vector)
        cuda.to_device(self.bonds.c)
        cuda.to_device(self.bonds.d)
        cuda.to_device(self.bonds.f_x)
        cuda.to_device(self.bonds.f_y)
```

```python
class ParticleSet:

     def compute_forces(self, bonds, cuda_available):
        """
        Compute particle forces
        """
        if cuda_available:
            compute_nodal_forces_cuda()
        else:
            self.f, _ = compute_nodal_forces(
                self.x,
                self.u,
                self.cell_volume,
                bonds.bondlist,
                bonds.d,
                bonds.c,
                bonds.f_x,
                bonds.f_y,
                bonds.constitutive_law.calculate_bond_damage,
                bonds.surface_correction_factors,
            )

```

```python
class EulerCromer(Integrator):

    def one_timestep(self, particles, simulation):
        """
        Update particle positions using an Euler-Cromer time integration scheme
        """
        if simulation.cuda_available:
            return euler_cromer_cuda()
        else:
            return euler_cromer(
                particles.f,
                particles.u,
                particles.v,
                particles.a,
                particles.node_density,
                particles.bc.flag,
                particles.bc.i_magnitude,
                particles.bc.unit_vector,
                simulation.damping,
                simulation.dt
            )
```

## CUDA Kernels

### Data structure

A neighbour list is a data structure used to efficiently store particles within a specified cut-off distance of each other. There are two common representations:

1. **Per-particle (neighbour list):** Each particle stores a list of its neighbours:

   ```python
   neighbours = [[j1, j2, j3], ...]  # shape: [n_particles, n_family_members]
   ```

   This results in a **nested loop** structure: loop over particles, then loop over each particles neighbours.

2. **Pairwise (bond list):** Each interaction is stored as a pair of particle indices:

   ```python
   bonds = [[i, j], ...]  # shape: [n_bonds, 2]
   ```

   This format allows the reduction of bond forces to particle forces to be carried out in a **single loop** over bonds.

A pairwise list is an efficient option on a CPU as it avoids nested for loops and redundant computations (each particle pair is visited only once). Bond forces can be computed in parallel (this is embarrassingly parallel) but the reduction of bond forces to particle forces must be done in serial, because multiple threads may simultaneously try to update the force on the same particle.

A per-particle neighbour list can avoid reductions, leading to better GPU performance, even at the cost of redundant calculations.


### GPU inefficiency with pairwise bond list

GPUs rely on **massively parallel execution**. In the pairwise representation, each thread can compute the force contribution of one bond, but these forces need to be **accumulated** back to the two participating particles.

```python
for bond in bonds:
    i, j = bond
    f = compute_force(i, j)
    forces[i] += f
    forces[j] -= f
```

This step requires **atomic operations** or some form of **reduction**, because multiple threads may simultaneously try to update the force on the same particle. Unfortunately, atomic operations:

* Are **serialised** on the GPU.
* Introduce **contention** when many bonds contribute to the same particle.
* Can severely degrade performance, especially when the number of neighbours is large or unevenly distributed.

### Alternative: Per-particle structure on GPU

In contrast, using a **per-particle neighbour list**, each GPU thread is assigned to a particle, and loops over its own neighbours:

```python
for i in range(n_particles):  # one thread per particle
    for j in neighbours[i]:
        forces[i] += compute_force(i, j)
```

This avoids the need for atomic operations because:

* Each thread **owns** the output slot (`forces[i]`).
* No force sharing is required during computation.

This approach trades some redundant computations (each pair will be visited twice) for massively parallel and conflict-free memory writes, which is often a worthwhile tradeoff on a GPU.


### `compute_nodal_forces_cuda()`

One of the key challenges in this work is implementing an efficient parallel reduction of bond forces to particle forces. To efficiently reduce bond forces to node forces we implement a parallel binary reduction. 

**Family list**

```python
for i in range(n_particles): 
    for j in neighbours[i]:
        particles.f[i] += bonds.f[i, j]
```

**Bond list**

```python
for bond in bonds:
    i, j = bond
    particles.f[i] += bond.f
    particles.f[j] -= bond.f
```

The performance improvement is examined in [this notebook](https://github.com/mark-hobbs/articles/blob/main/cuda/parallel-reduction.ipynb).

```python
@cuda.jit
def reduce_bond_forces(neighbourlist, f):
    """
    Reduce bond forces to particle forces
    """
    row = cuda.blockIdx.x
    tid = cuda.threadIdx.x
    n_cols = neighbourlist.shape[1]

    # Allocate shared memory for each thread to load one value
    sdata = cuda.shared.array(512, dtype=float32)  # Adjust size if needed

    val = 0.0
    if tid < n_cols:
        val = neighbourlist[row, tid]

    sdata[tid] = val
    cuda.syncthreads()

    s = cuda.blockDim.x // 2
    while s > 0:
        if tid < s and tid + s < n_cols:
            sdata[tid] += sdata[tid + s]
        cuda.syncthreads()
        s //= 2

    if tid == 0:
        f[row] = sdata[0]
```

The above can be written more cleanly using `cuTile`. In `cuTile` the execution unit is a tile block. Threads are implicit.

### `euler_cromer_cuda()`


### `ConstitutiveLaw`

We need to write a utility function (wrapper function) to create a CUDA-compatible material law. The material law is called with the signature `material_law(k_bond, s, d)`...

```python
class ConstitutiveLaw:

    def create_cuda_material_law(material_law):
        """
        Convert a Python material law function to a CUDA-compatible form
        
        Parameters:
        -----------
        material_law : callable
            A function that takes (k_bond, stretch, current_damage) 
            and returns updated damage
        
        Returns:
        --------
        cuda_material_law : ndarray
            A pre-compiled CUDA-compatible material law
        """
        pass
```

## Useful links

- [Numba for CUDA programmers](https://github.com/numba/nvidia-cuda-tutorial)
- [An even easier introduction to CUDA](https://developer.nvidia.com/blog/even-easier-introduction-cuda/)
- [CUDA programming guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html)
- [The CUDA programming model](https://developer.nvidia.com/blog/cuda-refresher-cuda-programming-model/)
- [Optimising parallel reduction in CUDA](https://developer.download.nvidia.com/assets/cuda/files/reduction.pdf)
- [numba.readthedocs](https://numba.readthedocs.io/en/0.60.0/)
- [https://numba.readthedocs.io/en/stable/cuda/index.html](https://numba.readthedocs.io/en/stable/cuda/index.html)
- [https://nvidia.github.io/numba-cuda/](https://nvidia.github.io/numba-cuda/)
- [GPU Programming in Pure Python - Bryce Adelstein Lelbach](https://www.youtube.com/watch?v=8utSRblGEB0&ab_channel=PyConUS)
- [1,001 ways to write CUDA kernels in Python](https://www.nvidia.com/en-us/on-demand/session/gtc25-S72449/)
  - Discusses efficient parallel reduction of arrays
- [CUDA Python](https://nvidia.github.io/cuda-python/latest/)
  - CUDA Python is the home for accessing NVIDIAâ€™s CUDA platform from Python.