---
layout: post
author: Mark Hobbs
title: Accelerating peridynamic simulations using Numba CUDA
draft: True
---

This post details the acceleration of the [`pypd`](https://github.com/mark-hobbs/pypd) package using `CUDA`. The goal is to ensure that the code requires no changes to the user interface while automatically detecting the presence of a CUDA-enabled device.

The majority of the simulation time is spent in two functions `particles.compute_forces` and `particles.update_forces`. These have both been optimised using `numba` in a manner similar to that employed when using `OpenMP`... i.e. shared memory type problem...

## The CUDA programming model

Before writing the CUDA kernel, we briefly review the CUDA programming model. The CUDA programming model provides an abstraction of the GPU architecture that acts as a bridge between an application and its possible implementation on GPU hardware. See this [post](https://developer.nvidia.com/blog/cuda-refresher-cuda-programming-model/) for further details.

```bash
CUDA Device Information:
----------------------------------------
Device Name:              b'Tesla T4'
Compute Capability:       (7, 5)

Memory:
Total Memory:             15.83 GB
Free Memory:              15.72 GB

Compute Resources:
Multiprocessors:          40
Max Threads per Block:    1024

Grid Limitations:
Max Grid Dimensions X:    2147483647
Max Grid Dimensions Y:    65535
Max Grid Dimensions Z:    65535

Additional Characteristics:
Warp Size:                32
Clock Rate:               1.59 GHz
Memory Clock Rate:        5.00 GHz

Estimated Parallel Computation Capacity:
Max Blocks per Multiprocessor: 2147483647
Total Potential Parallel Blocks: 85899345880

Launch Configuration Estimation:
For 1000000 work items:
Threads per Block: 1024
Blocks per Grid: 977
```

## `pypd`

Making some simplifications, setting up and running a simulation using `pypd` is broadly as follows:

```python
import pypd

particles = pypd.ParticleSet()
bonds = pypd.BondSet(particles)
model = pypd.Model(particles, bonds)
simulation = pypd.Simulation(n_time_steps=5000)
simulation.run(model)
```

When the simulation is run we need to detect if `CUDA` is available and then allocate memory to the device.

```python
from numba import cuda


class Simulation:

    def __init__(self, model):
        self.model = model
        self.cuda_available = self._is_cuda_available()
        if self.cuda_available:
            self.model._allocate_gpu_arrays()

    def _is_cuda_available(self):
        """
        Check if CUDA is available
        """
        return cuda.is_available()
```

Device memory allocations and data transfers should typically be done outside the CUDA kernel to:

- Minimise redundant data transfers
- Allow for more efficient memory management
- Enable potential reuse of device-allocated memory across multiple function calls

```python
from numba import cuda


class Model:
    
    def _allocate_gpu_arrays(self):
        from numba import cuda

        self.particles.x = cuda.to_device(self.particles.x)
        self.particles.x = cuda.to_device(self.particles.u)
```

### CUDA Kernel

```python
from numba import cuda, float64


@cuda.jit
def compute_bond_forces_kernel(
    x_d, u_d, cell_volume, bondlist_d, c_d, 
    surface_correction_factors_d, f_x_d, f_y_d, d_d, 
    cuda_material_law
):
    """
    CUDA kernel to compute bond forces with flexible material law
    """
    k_bond = cuda.grid(1)
    
    if k_bond < bondlist_d.shape[0]:
        node_i = bondlist_d[k_bond, 0]
        node_j = bondlist_d[k_bond, 1]

        xi_x = x_d[node_j, 0] - x_d[node_i, 0]
        xi_y = x_d[node_j, 1] - x_d[node_i, 1]

        xi_eta_x = xi_x + (u_d[node_j, 0] - u_d[node_i, 0])
        xi_eta_y = xi_y + (u_d[node_j, 1] - u_d[node_i, 1])

        xi = np.sqrt(xi_x**2 + xi_y**2)
        y = np.sqrt(xi_eta_x**2 + xi_eta_y**2)
        stretch = (y - xi) / xi

        # Use the flexible material law
        damage = cuda_material_law(k_bond, stretch, d_d[k_bond])
        
        # Ensure damage is between 0 and 1
        damage = max(0.0, min(damage, 1.0))
        d_d[k_bond] = damage

        f = (
            stretch 
            * c_d[k_bond] 
            * (1 - damage) 
            * cell_volume 
            * surface_correction_factors_d[k_bond]
        )
        
        f_x_d[k_bond] = f * xi_eta_x / y
        f_y_d[k_bond] = f * xi_eta_y / y

@cuda.jit
def reduce_forces_kernel(f_x_d, f_y_d, bondlist_d, node_force_d):
    """
    CUDA kernel to reduce bond forces to node forces
    """
    k_bond = cuda.grid(1)
    
    if k_bond < bondlist_d.shape[0]:
        node_i = bondlist_d[k_bond, 0]
        node_j = bondlist_d[k_bond, 1]

        cuda.atomic.add(node_force_d, (node_i, 0), f_x_d[k_bond])
        cuda.atomic.add(node_force_d, (node_j, 0), -f_x_d[k_bond])
        cuda.atomic.add(node_force_d, (node_i, 1), f_y_d[k_bond])
        cuda.atomic.add(node_force_d, (node_j, 1), -f_y_d[k_bond])
```

### `ParticleSet`

```python
class ParticleSet:

    def compute_nodal_forces(self, cell_volume, d, material_law):
        """
        Compute nodal forces using pre-allocated device memory
        
        Parameters:
        -----------
        cell_volume : float
            Volume of computational cell
        d : ndarray
            Current bond damage state
        material_law : callable
            Function to compute bond damage
        
        Returns:
        --------
        node_force : ndarray
            Computed nodal forces
        updated_d : ndarray
            Updated damage state
        """    
        # Allocate or reuse device arrays for this computation
        node_force_d = cuda.device_array((self.n_nodes, self.n_dimensions), dtype=np.float64)
        f_x_d = cuda.device_array(self.n_bonds, dtype=np.float64)
        f_y_d = cuda.device_array(self.n_bonds, dtype=np.float64)
        d_d = cuda.to_device(d)
        
        # Launch bond forces kernel
        compute_bond_forces_kernel[self.blocks_per_grid, self.threads_per_block](
            self.x_d, self.u_d, cell_volume, 
            self.bondlist_d, self.c_d, 
            self.surface_correction_factors_d, 
            f_x_d, f_y_d, d_d, 
            cuda_material_law
        )
        
        # Launch force reduction kernel
        reduce_forces_kernel[self.blocks_per_grid, self.threads_per_block](
            f_x_d, f_y_d, self.bondlist_d, node_force_d
        )
        
        # Copy results back to host
        node_force = node_force_d.copy_to_host()
        updated_d = d_d.copy_to_host()
        
        return node_force, updated_d
```

### `ConstitutiveLaw`

We need to write a utility function (wrapper function) to create a CUDA-compatible material law. The material law is called with the signature `material_law(k_bond, s, d)`...

```python
class ConstitutiveLaw:

    def create_cuda_material_law(material_law):
        """
        Convert a Python material law function to a CUDA-compatible form
        
        Parameters:
        -----------
        material_law : callable
            A function that takes (k_bond, stretch, current_damage) 
            and returns updated damage
        
        Returns:
        --------
        cuda_material_law : ndarray
            A pre-compiled CUDA-compatible material law
        """
        # Create a wrapper that can be used in CUDA kernel
        @cuda.jit(device=True)
        def cuda_material_law_wrapper(k_bond, stretch, current_damage):
            # We'll use a global array to pass the function
            return material_law_global[0](k_bond, stretch, current_damage)
        
        # Create a global reference to the original material law
        global material_law_global
        material_law_global = [material_law]
        
        return cuda_material_law_wrapper
```

## Useful links

- [Numba for CUDA programmers](https://github.com/numba/nvidia-cuda-tutorial)
- [An even easier introduction to CUDA](https://developer.nvidia.com/blog/even-easier-introduction-cuda/)
- [CUDA programming guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html)
- [The CUDA programming model](https://developer.nvidia.com/blog/cuda-refresher-cuda-programming-model/)